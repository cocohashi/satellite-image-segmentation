{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db424418-57f0-4c70-a23f-30d035512e58",
   "metadata": {},
   "source": [
    "## CUSTOM DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca86a00-446d-416b-90d0-85bc45fdd77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### IMPORT LIBRARIES\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8f9256-95f2-4a08-99b3-900f4b1c3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original image directory: /home/jovyan/data/dataset/semantic_drone_dataset/original_images\n",
      "\n",
      "Image Semantic directory: /home/jovyan/data/dataset/semantic_drone_dataset/label_images_semantic\n"
     ]
    }
   ],
   "source": [
    "## --- Configuration ---\n",
    "\n",
    "ROOT_PATH = '/home/jovyan/data/dataset/semantic_drone_dataset'\n",
    "IMAGES_PATH = os.path.join(ROOT_PATH, 'original_images')\n",
    "MASK_PATH = os.path.join(ROOT_PATH, 'label_images_semantic')\n",
    "SAMPLE_IMAGE = '001'\n",
    "\n",
    "print(f\"\"\"\n",
    "Original image directory: {IMAGES_PATH}\\n\n",
    "Image Semantic directory: {MASK_PATH}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ea255-f00e-4a5f-ac54-0afbea03df86",
   "metadata": {},
   "source": [
    "## LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9c0553-cd04-4d63-8c36-619e11f3c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, masks_dir: str, scale: float = 1.0, mask_suffix: str = ''):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    "        self.scale = scale\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(images_dir) if not file.startswith('.')]\n",
    "        if not self.ids:\n",
    "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(pil_img, scale, is_mask):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small, resized images would have no pixel'\n",
    "        pil_img = pil_img.resize((newW, newH), resample=Image.NEAREST if is_mask else Image.BICUBIC)\n",
    "        img_ndarray = np.asarray(pil_img)\n",
    "\n",
    "        if not is_mask:\n",
    "            if img_ndarray.ndim == 2:\n",
    "                img_ndarray = img_ndarray[np.newaxis, ...]\n",
    "            else:\n",
    "                img_ndarray = img_ndarray.transpose((2, 0, 1))\n",
    "\n",
    "            img_ndarray = img_ndarray / 255\n",
    "\n",
    "        return img_ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        ext = splitext(filename)[1]\n",
    "        if ext == '.npy':\n",
    "            return Image.fromarray(np.load(filename))\n",
    "        elif ext in ['.pt', '.pth']:\n",
    "            return Image.fromarray(torch.load(filename).numpy())\n",
    "        else:\n",
    "            return Image.open(filename)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.ids[idx]\n",
    "        mask_file = list(self.masks_dir.glob(name + self.mask_suffix + '.*'))\n",
    "        img_file = list(self.images_dir.glob(name + '.*'))\n",
    "\n",
    "        assert len(img_file) == 1, f'Either no image or multiple images found for the ID {name}: {img_file}'\n",
    "        assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\n",
    "        mask = self.load(mask_file[0])\n",
    "        img = self.load(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {name} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(img, self.scale, is_mask=False)\n",
    "        mask = self.preprocess(mask, self.scale, is_mask=True)\n",
    "\n",
    "        return {\n",
    "            'image': torch.as_tensor(img.copy()).float().contiguous(),\n",
    "            'mask': torch.as_tensor(mask.copy()).long().contiguous()\n",
    "        }\n",
    "\n",
    "class AerialDataset(BasicDataset):\n",
    "    def __init__(self, images_dir, masks_dir, scale=1):\n",
    "        super().__init__(images_dir, masks_dir, scale, mask_suffix='_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4434ed1-d103-40fc-8791-5f3bbff7624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_img = IMAGES_PATH\n",
    "dir_mask = MASK_PATH\n",
    "img_scale = 0.5\n",
    "\n",
    "try:\n",
    "    dataset = AerialDataset(dir_img, dir_mask, img_scale)\n",
    "except (AssertionError, RuntimeError):\n",
    "    dataset = BasicDataset(dir_img, dir_mask, img_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c92f384-ea4c-4c59-a7f2-b8dfd2892b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712c95c1-36a3-416e-afcb-0d2adfb21e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split into train / validation partitions\n",
    "val_percent = 0.2\n",
    "\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6422ed3-3758-4721-b46a-8f66043279a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create data loaders\n",
    "batch_size = 1\n",
    "\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eec964d-b975-49bf-aac6-edda1f011b38",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 295, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"/tmp/ipykernel_227/653537319.py\", line 51, in __getitem__\n    assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\nAssertionError: Either no mask or multiple masks found for the ID 178: []\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display image and label.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_features\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 295, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"/tmp/ipykernel_227/653537319.py\", line 51, in __getitem__\n    assert len(mask_file) == 1, f'Either no mask or multiple masks found for the ID {name}: {mask_file}'\nAssertionError: Either no mask or multiple masks found for the ID 178: []\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bca06f-6245-428c-8bd9-9c6b4015bea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
