{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6f8HuBbhqqE"
   },
   "source": [
    "Licensed under the Apache License, Version 2.0\n",
    "\n",
    "This python notebook shows how to reproduce and inspect the datasets and splits used in `In-Domain Representation Learning for Remote Sensing'\n",
    "by Maxim Neumann, André Susano Pinto, Xiaohua Zhai and Neil Houlsby. Pre-print\n",
    "available in [arXiv](https://arxiv.org/abs/1911.06721).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1Zss9bIiLT6"
   },
   "source": [
    "### Imports and code to define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_Mxo4AFhWNk"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCPoemuBisAq"
   },
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_PERCENT = 60\n",
    "VALIDATION_SPLIT_PERCENT = 20\n",
    "TEST_SPLIT_PERCENT = 20\n",
    "SO2SAT_VALIDATION_SUBSPLIT_PERCENT = 25\n",
    "\n",
    "class TfdsDataset:\n",
    "  def __init__(self, dataset_name, params={}, **kwargs):\n",
    "    is_so2sat = dataset_name.startswith(\"so2sat\")\n",
    "    is_bigearthnet = dataset_name.startswith(\"bigearthnet\")\n",
    "\n",
    "    self.name = dataset_name\n",
    "    self.is_multilabel = is_bigearthnet\n",
    "    self.builder = tfds.builder(dataset_name, **kwargs)\n",
    "\n",
    "    self.label_key = \"labels\" if is_bigearthnet else \"label\"\n",
    "    self.image_key = \"image\"\n",
    "    self.filename_key = \"sample_id\" if is_so2sat else \"filename\"\n",
    "\n",
    "    self._shuffle_buffer_size = params.get(\"shuffle_buffer_size\", 10000)\n",
    "    self._num_parallel_calls = params.get(\"num_preprocessing_threads\", 100)\n",
    "    self._drop_remainder = params.get(\"drop_remainder\", True)\n",
    "    self._ignore_errors = params.get(\"ignore_errors\", False)\n",
    "    self._prefetch = params.get(\"prefetch\", 1)\n",
    "\n",
    "    self.label_name_fn = self.builder.info.features[self.label_key].int2str\n",
    "    self.num_classes = self.builder.info.features[self.label_key].num_classes\n",
    "\n",
    "    if is_so2sat:\n",
    "      self._tfds_splits = dict(\n",
    "          train=f\"train\",\n",
    "          val=f\"validation[:{SO2SAT_VALIDATION_SUBSPLIT_PERCENT}%]\",\n",
    "          test=f\"validation[{SO2SAT_VALIDATION_SUBSPLIT_PERCENT}%:]\")\n",
    "      val_count = self.builder.info.splits[tfds.Split.VALIDATION].num_examples\n",
    "      self._num_samples_splits = dict(\n",
    "          train=self.builder.info.splits[tfds.Split.TRAIN].num_examples,\n",
    "          val=val_count * SO2SAT_VALIDATION_SUBSPLIT_PERCENT // 100,\n",
    "          test=val_count * (100-SO2SAT_VALIDATION_SUBSPLIT_PERCENT) // 100)\n",
    "    else:\n",
    "      self._tfds_splits = dict(\n",
    "          train=f\"train[:{TRAIN_SPLIT_PERCENT}%]\",\n",
    "          val=f\"train[{TRAIN_SPLIT_PERCENT}%:{TRAIN_SPLIT_PERCENT+VALIDATION_SPLIT_PERCENT}%]\",\n",
    "          test=f\"train[{TRAIN_SPLIT_PERCENT+VALIDATION_SPLIT_PERCENT}%:]\")\n",
    "      num_examples = self.builder.info.splits[tfds.Split.TRAIN].num_examples\n",
    "      self._num_samples_splits = dict(\n",
    "          train=num_examples * TRAIN_SPLIT_PERCENT // 100,\n",
    "          val=num_examples * VALIDATION_SPLIT_PERCENT // 100,\n",
    "          test=num_examples * TEST_SPLIT_PERCENT // 100)\n",
    "      \n",
    "  def _get_deterministic_dataset(self, split_name, for_eval, train_examples):\n",
    "    \"\"\"Creates a tf.data.Dataset composed of a deterministic set of examples.\"\"\"\n",
    "    # Don't shuffle to receive exactly the same split for reproducibility.\n",
    "    dataset = self.builder.as_dataset(\n",
    "        split=self._tfds_splits[split_name],\n",
    "        shuffle_files=False,\n",
    "        decoders={self.image_key: tfds.decode.SkipDecoding()})\n",
    "    num_samples = self._num_samples_splits[split_name]\n",
    "\n",
    "    if not for_eval and train_examples:\n",
    "      dataset = dataset.take(train_examples)\n",
    "      num_samples = train_examples\n",
    "\n",
    "    return dataset, num_samples\n",
    "\n",
    "  def get_filenames(self, split_name, train_examples=None, for_eval=False):\n",
    "    dataset, num_samples = self._get_deterministic_dataset(split_name, for_eval, train_examples)\n",
    "    def _get(example):\n",
    "      fname = example[self.filename_key].numpy()\n",
    "      if np.issubdtype(type(fname), np.signedinteger):\n",
    "        fname = bytes(str(fname), encoding=\"utf-8\")\n",
    "      return fname\n",
    "    return list([_get(x) for x in dataset])\n",
    "    \n",
    "  def get_tf_data(self, split_name, batch_size, preprocess_fn=None,\n",
    "                  for_eval=False, train_examples=None, epochs=None):\n",
    "    \"\"\"Creates a tf.data.Dataset with features (label, image, filename).\"\"\"\n",
    "    dataset, num_samples = self._get_deterministic_dataset(split_name, for_eval, train_examples)\n",
    "      \n",
    "    # Cache the whole dataset if it's smaller than 150K examples.\n",
    "    if not for_eval and num_samples <= 150000:\n",
    "      dataset = dataset.cache()\n",
    "\n",
    "    # Repeats data `epochs` time or indefinitely if `epochs` is None.\n",
    "    if epochs is None or epochs > 1:\n",
    "      dataset = dataset.repeat(epochs)\n",
    "\n",
    "    if not for_eval and self._shuffle_buffer_size > 1:\n",
    "      dataset = dataset.shuffle(self._shuffle_buffer_size)\n",
    "\n",
    "    def prepare_example(example):\n",
    "      image_decoder = self.builder.info.features[self.image_key].decode_example\n",
    "      # Rename features to common names.\n",
    "      example = {\n",
    "          \"image\": image_decoder(example[self.image_key]),\n",
    "          \"label\": example[self.label_key],\n",
    "          \"filename\": example[self.filename_key],\n",
    "      }\n",
    "      if self.is_multilabel:\n",
    "        example[\"label\"] = tf.reduce_max(tf.one_hot(example[\"label\"],\n",
    "                                                    depth=self.num_classes,\n",
    "                                                    dtype=tf.int64), axis=0)\n",
    "      if preprocess_fn:\n",
    "        example = preprocess_fn(example)\n",
    "      return example\n",
    "\n",
    "    dataset = dataset.map(prepare_example, self._num_parallel_calls)\n",
    "    if self._ignore_errors:  # Ignore images with errors.\n",
    "      dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
    "    dataset = dataset.batch(batch_size, self._drop_remainder)\n",
    "    dataset = dataset.prefetch(self._prefetch)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_fn(data, size=224, input_range=(0.0, 1.0)):\n",
    "  image = data[\"image\"]\n",
    "  image = tf.image.resize(image, [size, size])\n",
    "  image = tf.cast(image, tf.float32) / 255.0\n",
    "  image = image * (input_range[1] - input_range[0]) + input_range[0]\n",
    "  data[\"image\"] = image\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0O2eSq9EhwzA"
   },
   "outputs": [],
   "source": [
    "def visualize(ds, figsize=(17, 17)):\n",
    "  batch_size = 16\n",
    "  train = ds.get_tf_data(\"val\", batch_size,  preprocess_fn=preprocess_fn)\n",
    "  xx = next(train.make_one_shot_iterator())\n",
    "  print(f\"Dataset: {ds.name}\")\n",
    "  print(\"Images: \", xx[\"image\"].shape, stats_str(xx[\"image\"]))\n",
    "  print(\"Labels: \", xx[\"label\"].shape, stats_str(xx[\"label\"]))\n",
    "  plt.figure(figsize=figsize)\n",
    "  for i in range(batch_size):\n",
    "    plt.subplot(4, 4, 1+i)\n",
    "    plt.imshow(xx[\"image\"][i])\n",
    "    if ds.is_multilabel:\n",
    "      labels = [ds.label_name_fn(lid) for lid, value in enumerate(xx[\"label\"][i]) if value]\n",
    "      plt.title(\"\\n\".join(labels))\n",
    "    else:\n",
    "      plt.title(ds.label_name_fn(xx[\"label\"][i]))\n",
    "  plt.show()\n",
    "\n",
    "def stats_str(arr, f=None, with_median=False, with_count=False):\n",
    "  \"\"\"Returns a string with main stats info about the given array.\n",
    "\n",
    "  By default, the string has the form: \"mean +/- standard_deviation [min..max]\"\n",
    "  values of the data array.\n",
    "\n",
    "  Args:\n",
    "    arr: array-like\n",
    "    f: str\n",
    "    with_median: boolean\n",
    "    with_count: boolean\n",
    "  Returns:\n",
    "    stats_str: str\n",
    "  \"\"\"\n",
    "  if arr is None or (isinstance(arr, (list, tuple)) and not arr):\n",
    "    return \"[empty]\"\n",
    "  if not isinstance(arr, np.ndarray):\n",
    "    try:\n",
    "      arr = arr.numpy()  # If arr is a TF-2 tensor.\n",
    "    except AttributeError:\n",
    "      pass\n",
    "    try:\n",
    "      arr = np.concatenate(arr).ravel()  # to deal with different length lists\n",
    "    except ValueError:\n",
    "      arr = np.array(arr)\n",
    "  if f is None:\n",
    "    f = \"{:.3f}\"\n",
    "  if with_median:\n",
    "    median = (\" median: \" + f).format(np.median(arr))\n",
    "  else:\n",
    "    median = \"\"\n",
    "  count = \" n: {:,}\".format(len(arr)) if with_count else \"\"\n",
    "  pm = \"+/-\"  # this one doesn't work: u' \\u00B1'\n",
    "  if arr.dtype.kind in [\"i\", \"u\"]:\n",
    "    return (f + pm + f + \" [{}..{}]\").format(arr.mean(), arr.std(), arr.min(),\n",
    "                                             arr.max()) + median + count\n",
    "  return (f + pm + f + \" [\" + f + \"..\" + f + \"]\").format(\n",
    "      arr.mean(), arr.std(), arr.min(), arr.max()) + median + count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlI5jYBTiRF1"
   },
   "source": [
    "## Inspect a specific dataset\n",
    "\n",
    "Attention: many datasets take long to build and may required multiple days to download. So2Sat must be setup manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvNv3aFbiGNg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 89.91 MiB (download: 89.91 MiB, generated: Unknown size, total: 89.91 MiB) to /home/hasier/tensorflow_datasets/eurosat/rgb/2.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/89 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]B/s]\u001b[A\n",
      "Dl Size...:   1%|          | 1/89 [00:00<00:52,  1.69 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   2%|▏         | 2/89 [00:00<00:51,  1.69 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]B/s]\u001b[A\n",
      "Dl Size...:   3%|▎         | 3/89 [00:00<00:16,  5.12 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   4%|▍         | 4/89 [00:00<00:16,  5.12 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]B/s]\u001b[A\n",
      "Dl Size...:   6%|▌         | 5/89 [00:00<00:10,  8.15 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   7%|▋         | 6/89 [00:00<00:10,  8.15 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   8%|▊         | 7/89 [00:00<00:10,  8.15 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]B/s]\u001b[A\n",
      "Dl Size...:   9%|▉         | 8/89 [00:00<00:06, 12.78 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  10%|█         | 9/89 [00:00<00:06, 12.78 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  11%|█         | 10/89 [00:01<00:05, 14.45 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  12%|█▏        | 11/89 [00:01<00:05, 14.45 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  13%|█▎        | 12/89 [00:01<00:05, 14.45 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  15%|█▍        | 13/89 [00:01<00:04, 17.83 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  16%|█▌        | 14/89 [00:01<00:04, 17.83 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  17%|█▋        | 15/89 [00:01<00:04, 17.83 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  18%|█▊        | 16/89 [00:01<00:03, 20.44 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  19%|█▉        | 17/89 [00:01<00:03, 20.44 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  20%|██        | 18/89 [00:01<00:03, 20.44 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  21%|██▏       | 19/89 [00:01<00:03, 18.08 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  22%|██▏       | 20/89 [00:01<00:03, 18.08 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  24%|██▎       | 21/89 [00:01<00:03, 18.08 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  25%|██▍       | 22/89 [00:01<00:03, 19.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  26%|██▌       | 23/89 [00:01<00:03, 19.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  27%|██▋       | 24/89 [00:01<00:03, 19.35 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  28%|██▊       | 25/89 [00:01<00:03, 20.84 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  29%|██▉       | 26/89 [00:01<00:03, 20.84 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  30%|███       | 27/89 [00:01<00:02, 20.84 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  31%|███▏      | 28/89 [00:01<00:02, 22.62 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  33%|███▎      | 29/89 [00:01<00:02, 22.62 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  34%|███▎      | 30/89 [00:01<00:02, 22.62 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  35%|███▍      | 31/89 [00:01<00:02, 23.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...:  36%|███▌      | 32/89 [00:01<00:02, 23.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  37%|███▋      | 33/89 [00:02<00:02, 23.35 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  38%|███▊      | 34/89 [00:02<00:02, 23.01 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  39%|███▉      | 35/89 [00:02<00:02, 23.01 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  40%|████      | 36/89 [00:02<00:02, 23.01 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  42%|████▏     | 37/89 [00:02<00:02, 23.89 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  43%|████▎     | 38/89 [00:02<00:02, 23.89 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  44%|████▍     | 39/89 [00:02<00:02, 23.89 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  45%|████▍     | 40/89 [00:02<00:02, 24.12 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  46%|████▌     | 41/89 [00:02<00:01, 24.12 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  47%|████▋     | 42/89 [00:02<00:01, 24.12 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  48%|████▊     | 43/89 [00:02<00:01, 24.19 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  49%|████▉     | 44/89 [00:02<00:01, 24.19 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  51%|█████     | 45/89 [00:02<00:01, 24.19 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  52%|█████▏    | 46/89 [00:02<00:01, 24.08 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  53%|█████▎    | 47/89 [00:02<00:01, 24.08 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  54%|█████▍    | 48/89 [00:02<00:01, 24.08 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  55%|█████▌    | 49/89 [00:02<00:01, 24.53 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  56%|█████▌    | 50/89 [00:02<00:01, 24.53 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  57%|█████▋    | 51/89 [00:02<00:01, 24.53 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  58%|█████▊    | 52/89 [00:02<00:01, 24.51 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  60%|█████▉    | 53/89 [00:02<00:01, 24.51 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  61%|██████    | 54/89 [00:02<00:01, 24.51 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  62%|██████▏   | 55/89 [00:02<00:01, 24.23 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  63%|██████▎   | 56/89 [00:02<00:01, 24.23 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...:  64%|██████▍   | 57/89 [00:02<00:01, 24.23 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  65%|██████▌   | 58/89 [00:03<00:01, 25.04 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  66%|██████▋   | 59/89 [00:03<00:01, 25.04 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  67%|██████▋   | 60/89 [00:03<00:01, 25.04 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  69%|██████▊   | 61/89 [00:03<00:01, 25.37 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  70%|██████▉   | 62/89 [00:03<00:01, 25.37 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  71%|███████   | 63/89 [00:03<00:01, 25.37 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  72%|███████▏  | 64/89 [00:03<00:00, 25.77 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  73%|███████▎  | 65/89 [00:03<00:00, 25.77 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  74%|███████▍  | 66/89 [00:03<00:00, 25.77 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  75%|███████▌  | 67/89 [00:03<00:00, 25.84 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  76%|███████▋  | 68/89 [00:03<00:00, 25.84 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  78%|███████▊  | 69/89 [00:03<00:00, 25.84 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  79%|███████▊  | 70/89 [00:03<00:00, 24.21 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  80%|███████▉  | 71/89 [00:03<00:00, 24.21 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  81%|████████  | 72/89 [00:03<00:00, 24.21 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  82%|████████▏ | 73/89 [00:03<00:00, 23.99 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  83%|████████▎ | 74/89 [00:03<00:00, 23.99 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  84%|████████▍ | 75/89 [00:03<00:00, 23.99 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  85%|████████▌ | 76/89 [00:03<00:00, 23.85 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  87%|████████▋ | 77/89 [00:03<00:00, 23.85 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  88%|████████▊ | 78/89 [00:03<00:00, 23.85 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  89%|████████▉ | 79/89 [00:03<00:00, 23.86 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  90%|████████▉ | 80/89 [00:03<00:00, 23.86 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Size...:  91%|█████████ | 81/89 [00:03<00:00, 23.86 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  92%|█████████▏| 82/89 [00:04<00:00, 24.25 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Size...:  93%|█████████▎| 83/89 [00:04<00:00, 24.25 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Size...:  94%|█████████▍| 84/89 [00:04<00:00, 24.25 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  96%|█████████▌| 85/89 [00:04<00:00, 25.39 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Size...:  97%|█████████▋| 86/89 [00:04<00:00, 25.39 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Size...:  98%|█████████▊| 87/89 [00:04<00:00, 25.39 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\n",
      "Dl Size...:  99%|█████████▉| 88/89 [00:04<00:00, 25.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Size...: 100%|██████████| 89/89 [00:04<00:00, 25.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:04<00:00,  4.32s/ url]\n",
      "Dl Size...: 100%|██████████| 89/89 [00:04<00:00, 25.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:04<00:00,  4.32s/ url]\n",
      "Dl Size...: 100%|██████████| 89/89 [00:04<00:00, 25.35 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/1 [00:04<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  4.32s/ url]5s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 89/89 [00:09<00:00, 25.35 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 1/1 [00:09<00:00,  9.06s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 89/89 [00:09<00:00,  9.82 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:09<00:00,  9.07s/ url]\n",
      "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]\n",
      "Generating train examples...:   0%|          | 0/27000 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating train examples...:   0%|          | 111/27000 [00:00<00:24, 1109.27 examples/s]\u001b[A\n",
      "Generating train examples...:   3%|▎         | 704/27000 [00:00<00:06, 3940.86 examples/s]\u001b[A\n",
      "Generating train examples...:   5%|▍         | 1304/27000 [00:00<00:05, 4879.34 examples/s]\u001b[A\n",
      "Generating train examples...:   7%|▋         | 1917/27000 [00:00<00:04, 5372.64 examples/s]\u001b[A\n",
      "Generating train examples...:   9%|▉         | 2501/27000 [00:00<00:04, 5538.50 examples/s]\u001b[A\n",
      "Generating train examples...:  12%|█▏        | 3126/27000 [00:00<00:04, 5777.79 examples/s]\u001b[A\n",
      "Generating train examples...:  14%|█▍        | 3747/27000 [00:00<00:03, 5916.78 examples/s]\u001b[A\n",
      "Generating train examples...:  16%|█▌        | 4369/27000 [00:00<00:03, 6010.12 examples/s]\u001b[A\n",
      "Generating train examples...:  18%|█▊        | 4991/27000 [00:00<00:03, 6073.00 examples/s]\u001b[A\n",
      "Generating train examples...:  21%|██        | 5599/27000 [00:01<00:03, 6074.66 examples/s]\u001b[A\n",
      "Generating train examples...:  23%|██▎       | 6217/27000 [00:01<00:03, 6104.37 examples/s]\u001b[A\n",
      "Generating train examples...:  25%|██▌       | 6839/27000 [00:01<00:03, 6137.23 examples/s]\u001b[A\n",
      "Generating train examples...:  28%|██▊       | 7463/27000 [00:01<00:03, 6166.13 examples/s]\u001b[A\n",
      "Generating train examples...:  30%|██▉       | 8094/27000 [00:01<00:03, 6209.10 examples/s]\u001b[A\n",
      "Generating train examples...:  32%|███▏      | 8715/27000 [00:01<00:03, 6059.96 examples/s]\u001b[A\n",
      "Generating train examples...:  35%|███▍      | 9330/27000 [00:01<00:02, 6082.76 examples/s]\u001b[A\n",
      "Generating train examples...:  37%|███▋      | 9939/27000 [00:01<00:03, 5078.42 examples/s]\u001b[A\n",
      "Generating train examples...:  39%|███▉      | 10545/27000 [00:01<00:03, 5334.45 examples/s]\u001b[A\n",
      "Generating train examples...:  41%|████▏     | 11149/27000 [00:01<00:02, 5524.11 examples/s]\u001b[A\n",
      "Generating train examples...:  44%|████▎     | 11756/27000 [00:02<00:02, 5676.40 examples/s]\u001b[A\n",
      "Generating train examples...:  46%|████▌     | 12369/27000 [00:02<00:02, 5803.65 examples/s]\u001b[A\n",
      "Generating train examples...:  48%|████▊     | 12982/27000 [00:02<00:02, 5896.05 examples/s]\u001b[A\n",
      "Generating train examples...:  50%|█████     | 13603/27000 [00:02<00:02, 5987.84 examples/s]\u001b[A\n",
      "Generating train examples...:  53%|█████▎    | 14208/27000 [00:02<00:02, 5971.96 examples/s]\u001b[A\n",
      "Generating train examples...:  55%|█████▍    | 14826/27000 [00:02<00:02, 6032.00 examples/s]\u001b[A\n",
      "Generating train examples...:  57%|█████▋    | 15433/27000 [00:02<00:01, 6027.96 examples/s]\u001b[A\n",
      "Generating train examples...:  59%|█████▉    | 16056/27000 [00:02<00:01, 6087.30 examples/s]\u001b[A\n",
      "Generating train examples...:  62%|██████▏   | 16675/27000 [00:02<00:01, 6115.94 examples/s]\u001b[A\n",
      "Generating train examples...:  64%|██████▍   | 17288/27000 [00:02<00:01, 6057.72 examples/s]\u001b[A\n",
      "Generating train examples...:  66%|██████▋   | 17895/27000 [00:03<00:01, 5900.73 examples/s]\u001b[A\n",
      "Generating train examples...:  68%|██████▊   | 18487/27000 [00:03<00:01, 5806.27 examples/s]\u001b[A\n",
      "Generating train examples...:  71%|███████   | 19083/27000 [00:03<00:01, 5848.61 examples/s]\u001b[A\n",
      "Generating train examples...:  73%|███████▎  | 19703/27000 [00:03<00:01, 5951.53 examples/s]\u001b[A\n",
      "Generating train examples...:  75%|███████▌  | 20299/27000 [00:03<00:01, 5931.65 examples/s]\u001b[A\n",
      "Generating train examples...:  77%|███████▋  | 20919/27000 [00:03<00:01, 6010.66 examples/s]\u001b[A\n",
      "Generating train examples...:  80%|███████▉  | 21522/27000 [00:03<00:00, 6016.28 examples/s]\u001b[A\n",
      "Generating train examples...:  82%|████████▏ | 22143/27000 [00:03<00:00, 6072.82 examples/s]\u001b[A\n",
      "Generating train examples...:  84%|████████▍ | 22755/27000 [00:03<00:00, 6086.52 examples/s]\u001b[A\n",
      "Generating train examples...:  87%|████████▋ | 23364/27000 [00:04<00:00, 6081.69 examples/s]\u001b[A\n",
      "Generating train examples...:  89%|████████▉ | 23973/27000 [00:04<00:00, 6066.14 examples/s]\u001b[A\n",
      "Generating train examples...:  91%|█████████ | 24580/27000 [00:04<00:00, 6039.30 examples/s]\u001b[A\n",
      "Generating train examples...:  93%|█████████▎| 25185/27000 [00:04<00:00, 5925.05 examples/s]\u001b[A\n",
      "Generating train examples...:  95%|█████████▌| 25779/27000 [00:04<00:00, 5780.85 examples/s]\u001b[A\n",
      "Generating train examples...:  98%|█████████▊| 26358/27000 [00:04<00:00, 5600.76 examples/s]\u001b[A\n",
      "Generating train examples...: 100%|█████████▉| 26920/27000 [00:04<00:00, 5573.60 examples/s]\u001b[A\n",
      "                                                                                            \u001b[A\n",
      "Shuffling /home/hasier/tensorflow_datasets/eurosat/rgb/2.0.0.incompleteQVNDF2/eurosat-train.tfrecord*...:   0%|          | 0/27000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling /home/hasier/tensorflow_datasets/eurosat/rgb/2.0.0.incompleteQVNDF2/eurosat-train.tfrecord*...:  58%|█████▊    | 15661/27000 [00:00<00:00, 156598.58 examples/s]\u001b[A\n",
      "                                                                                                                                                                          \u001b[A\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset eurosat downloaded and prepared to /home/hasier/tensorflow_datasets/eurosat/rgb/2.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PrefetchDataset' object has no attribute 'make_one_shot_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Visualize examples\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TfdsDataset(DATASET_NAME)\n\u001b[0;32m----> 9\u001b[0m visualize(dataset)\n",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(ds, figsize)\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mget_tf_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size,  preprocess_fn\u001b[38;5;241m=\u001b[39mpreprocess_fn)\n\u001b[0;32m----> 4\u001b[0m xx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_one_shot_iterator\u001b[49m())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages: \u001b[39m\u001b[38;5;124m\"\u001b[39m, xx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, stats_str(xx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute 'make_one_shot_iterator'"
     ]
    }
   ],
   "source": [
    "# Prepare and load a dataset\n",
    "POSSIBLE_DATASET_NAMES = [\"bigearthnet\", \"eurosat\", \"resisc45\", \"so2sat\", \"uc_merced\"]\n",
    "DATASET_NAME = \"eurosat\"\n",
    "\n",
    "tfds.load(DATASET_NAME);  # This will trigger downloading and preparing the dataset.\n",
    "\n",
    "# Visualize examples\n",
    "dataset = TfdsDataset(DATASET_NAME)\n",
    "visualize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCca62f4iXfr"
   },
   "source": [
    "## Dump the splits used in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SG7OkXLZiJaf"
   },
   "outputs": [],
   "source": [
    "# Dump the specific splits used on the dataset.\n",
    "dataset = TfdsDataset(DATASET_NAME)\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "  filenames = dataset.get_filenames(split)\n",
    "  output = f\"/tmp/{dataset.name}-{split}.txt\"\n",
    "  with open(output, \"wb\") as f:\n",
    "    f.write(b\"\\n\".join(filenames))\n",
    "  print(f\"Wrote: {output}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RSR datasets",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
